## 7.28.20 Class Exercise:<br/>
-  One-hot encoding is an ecoding method that represents words (or parts of words) in 1 demensional vectors. Each position in the vector represents a word/word fragment in the vocabulary. Each position in the vector contains a zero if the word does not match the associated entry in the vocabular or a one if the word does match the associated voacbulary entry. In vectorizing a corpus of words, one-hot encoding in not a good choice as the encoder because the vocabulary is likely very long so you will end up with equally long vectors. These vectors also indicate nothing about the relative meaning of each word since vocabulary organizes words based on frequency of appearance in the tokenizing data. In such cases when the meaning of words and comparing the similarity of words is important, a superior method of encoding is four dimensional embedding (4DE). 4DE represents each word as a 4-dimensional vector. These vectors can be used to find similarities in groups of words. For exmaple, consider the following:<br/>
Man --> Woman as King --> _____. The answer is queen and by training a model using 4DE data, it can predict this using a little "mathemagic". We know there is a relation between the words man and woman so if we subtract their 4D vectors, we get a vector that describes this relation. Now, subtract the other vectors of words from the vocabularly from the vector for king. We can then compare these subtracted vectors to the one we got from man and woman. The most similar vector will be the one from subracting queen from king ([Example source](https://www.youtube.com/watch?v=EEk6OiOOT2c)).
