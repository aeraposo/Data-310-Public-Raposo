## 7.29.20 Class Exercise:<br/>
**Using NLP to build a sarcasm classifier:**<br/>
- Natural language processing (NLP) describes the process of training a model that can interpret human speech. Some of the capabilities of NLP include language translation, text generation, and more. This week, we built an RNN that can detect sarcasm in news headlines. We achieved this by tokenizing words in a dataset of news headlines labeled either sarcastic (1) or not sarcastic (0) to turn sentences into sequences of numbers, padded these sequences with zeros to make them all the same length, and built an RNN (uses an embedding layer). After fitting the model, I gave it a spin on the following examples (green check = correct prediction, red x = incorrect prediction).<br/>

✅[1.3710256e-08] = ~0 - "The child-care industry is on the brink of collapse. Congress must rescue it." ([Washington Post](https://www.washingtonpost.com/))<br/>
✅[2.8298592e-07] = ~0 - "Protests live updates: NYPD facing questions as demonstrations continue." ([Washington Post](https://www.washingtonpost.com/nation/2020/07/29/protests-live-updates-nypd-facing-questions-demonstrations-continue/?hpid=hp_no-name_hp-in-the-news%3Apage%2Fin-the-news))<br/>
✅[3.2580751e-01] = ~0 - "US announces plans to remove 12,000 troops from Germany, with half coming home." ([Fox News](https://www.foxnews.com/world/us-announces-plans-to-send-troops-home))<br/>
✅[7.1725477e-03] = ~0 - "Justice Department to expand federal law enforcement presence in Midwestern cities." ([POLITICO](https://www.politico.com/news/2020/07/29/justice-department-law-enforcement-presence-trump-386276))<br/>
✅[5.6891394e-09] = ~0 - "These Microbes May Have Survived 100 Million Years Beneath the Seafloor." ([The New York Times](https://www.nytimes.com/2020/07/28/science/microbes-100-million-years-old.html))<br/>
❌[1.5391873e-07] = ~0 - "Trump’s Summer of Love Is a Distant Memory Now." ([POLITICO](https://www.politico.com/news/magazine/2020/07/27/trump-rallies-past-381315))<br/>
✅[9.9453628e-01] = ~1 - "Michael Goodwin: Barr eats Nadler's lunch during testimony." ([Fox News](https://www.foxnews.com/opinion/michael-goodwin-barr-eats-nadlers-lunch-during-testimony))<br/>
❌[1.6046785e-07] = ~0 - "A Republican Panic Button." ([The New York Times](https://www.nytimes.com/2020/07/27/us/politics/a-republican-panic-button.html))<br/>
✅[8.8728958e-01] = ~1 - "Why You’ll See Two Figures for G.D.P. Decline: Very Big, and Huge." ([The New York Times](https://www.nytimes.com/2020/07/29/business/economy/us-gdp-report.html))<br/>
❌[2.1694852e-05] = ~0 - "Joe Biden takes a non-virtual moment to muse on his basement, apocalyptic ads and being Trump’s ‘antithesis’." ([The Washington Post](https://www.washingtonpost.com/politics/joe-biden-takes-a-non-virtual-moment-to-muse-on-his-basement-apocalyptic-ads-and-being-trumps-antithesis/2020/07/29/801405f2-d133-11ea-8c55-61e7fa5e82ab_story.html))<br/>

Not too bad! The training and testing scores showed that the model became overfit so these mixed results were expected (likely due to the very large vocabulary). Particularly model had a had time identifying sarcastic headlines but the cases that were missed are not super obvious; if I didn't know the contexts, I would probably label them as not sarcastic too (I'm actually questioning if the ones it counted as not sarcastic really are, I'm really bad at picking up on sarcasm).<br/>
